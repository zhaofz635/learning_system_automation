import pandas as pd
import numpy as np
import json
import joblib
import os
import re
import math
from datetime import datetime
import requests
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
import cv2
from skimage.feature import graycomatrix, graycoprops

# ========================================
# ğŸ”§ NLTK è·¯å¾„ä¸èµ„æºç®¡ç†ï¼ˆå…³é”®ä¿®å¤ï¼‰
# ========================================

# å®šä¹‰ä¸ GitHub Actions ä¸€è‡´çš„è·¯å¾„
NLTK_DATA_PATH = '/tmp/nltk_data'

def setup_nltk():
    """ç¡®ä¿ NLTK èƒ½æ‰¾åˆ°æ•°æ®ç›®å½•"""
    # ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡
    custom_path = os.getenv('NLTK_DATA')
    if custom_path and custom_path not in nltk.data.path:
        nltk.data.path.insert(0, custom_path)
    
    # æ·»åŠ é»˜è®¤è·¯å¾„
    if NLTK_DATA_PATH not in nltk.data.path:
        nltk.data.path.insert(0, NLTK_DATA_PATH)
    
    # è°ƒè¯•è¾“å‡º
    print("ğŸ” NLTK data search paths:", nltk.data.path)

def download_nltk_resources():
    """ä¸‹è½½å¿…è¦èµ„æºåˆ°æŒ‡å®šç›®å½•"""
    try:
        nltk.data.find('tokenizers/punkt')
        print("âœ… punkt å·²å­˜åœ¨")
    except LookupError:
        print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½ punkt åˆ° {NLTK_DATA_PATH}")
        nltk.download('punkt', download_dir=NLTK_DATA_PATH, quiet=False)

    try:
        nltk.data.find('corpora/stopwords')
        print("âœ… stopwords å·²å­˜åœ¨")
    except LookupError:
        print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½ stopwords åˆ° {NLTK_DATA_PATH}")
        nltk.download('stopwords', download_dir=NLTK_DATA_PATH, quiet=False)

# æ‰§è¡Œåˆå§‹åŒ–
setup_nltk()
download_nltk_resources()

# ========================================
# ğŸ“š æœ¯è¯­åº“
# ========================================

class AcademicTermBank:
    def __init__(self, path="academic_terms.txt"):
        self.terms = self.load_terms(path)

    def load_terms(self, path):
        try:
            with open(path, 'r', encoding='utf-8') as f:
                terms = set(f.read().splitlines())
            print(f"æœ¯è¯­åº“åŠ è½½æˆåŠŸï¼Œæœ¯è¯­æ€»æ•°: {len(terms)}")
            return terms
        except FileNotFoundError:
            print(f"æœªæ‰¾åˆ°æœ¯è¯­åº“æ–‡ä»¶: {path}, ä½¿ç”¨ç©ºæœ¯è¯­åº“")
            return set()

# ========================================
# ğŸ–¼ï¸ è¡¨æ ¼/å›¾åƒå¤æ‚åº¦åˆ†æå™¨
# ========================================

class TableComplexityAnalyzer:
    def __init__(self, weight_structured=0.6, weight_image=0.4):
        self.weights = {'structured': weight_structured, 'image': weight_image}
        self.sub_weights = {'row_complexity': 0.4, 'method_diversity': 0.3, 'column_variety': 0.3}
        self.vectorizer = TfidfVectorizer()
        self._prepare_tfidf()

    def _prepare_tfidf(self):
        IMPORTANT_COLUMNS = ['metric', 'formula', 'threshold', 'method']
        self.vectorizer.fit(IMPORTANT_COLUMNS)

    def analyze_entry(self, entry):
        struct_score = self._calc_structured_features(entry)
        image_score = self._calc_image_features(entry.get('image_path', ''))
        score = np.round(struct_score * self.weights['structured'] + image_score * self.weights['image'], 2)
        if score == 0 and 'text' in entry:
            text = entry['text']
            words = word_tokenize(text)
            if len(words) > 100:
                score = 0.5
        return score

    def _calc_structured_features(self, entry):
        if 'structured_data' not in entry or not entry['structured_data']:
            return 0
        table = entry['structured_data']
        rows = table.get('rows', [])
        row_comp = sum(len(r.get('calculation', {}).get('methods', [])) for r in rows) / len(rows) if rows else 0
        methods = {m['name'] for r in rows for m in r.get('calculation', {}).get('methods', [])}
        method_div = len(methods)
        columns = table.get('columns', [])
        if columns:
            X = self.vectorizer.transform(columns)
            col_var = X.mean(axis=1).sum() * 0.5
        else:
            col_var = 0
        return (row_comp * self.sub_weights['row_complexity'] +
                method_div * self.sub_weights['method_diversity'] +
                col_var * self.sub_weights['column_variety'])

    def _calc_image_features(self, image_path):
        if not image_path or not os.path.exists(image_path):
            return 0.0
        try:
            img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
            if img is None:
                return 0.0
            img = cv2.resize(img, (256, 256))
            img_eq = cv2.equalizeHist(img)
            img_bin = cv2.adaptiveThreshold(img_eq, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)
            glcm = graycomatrix(img_bin, distances=[3, 5], angles=[0, np.pi/4, np.pi/2, 3*np.pi/4],
                                levels=256, symmetric=True, normed=True)
            contrast = graycoprops(glcm, 'contrast').mean()
            dissimilarity = graycoprops(glcm, 'dissimilarity').mean()
            return contrast * 0.7 + dissimilarity * 0.3
        except Exception as e:
            print(f"å›¾åƒå¤„ç†é”™è¯¯: {str(e)}")
            return 0.0

# ========================================
# ğŸ“Š è®¡ç®—å„é¡¹å¤æ‚åº¦
# ========================================

def calculate_language_complexity(text, term_bank):
    words = word_tokenize(text)
    sentences = sent_tokenize(text)
    avg_sentence_length = len(words) / max(len(sentences), 1)
    stop_words = set(stopwords.words("english"))
    non_stop_words = [w for w in words if w.lower() not in stop_words]
    non_stop_ratio = len(non_stop_words) / max(len(words), 1)
    long_words = [w for w in words if len(w) >= 6]
    long_word_ratio = len(long_words) / max(len(words), 1)
    tokens = [w.lower() for w in words]
    ngrams = set()
    for n in range(1, 4):
        for i in range(len(tokens) - n + 1):
            ngrams.add(' '.join(tokens[i:i + n]))
    matched_terms = ngrams & term_bank.terms
    term_ratio = min(1.0, len(matched_terms) / 8.0)
    sentence_length_factor = min(avg_sentence_length / 20, 1.0)
    w1, w2, w3, w4 = 0.3, 0.3, 0.3, 0.1
    complex_word_ratio = (
            w1 * non_stop_ratio +
            w2 * long_word_ratio +
            w3 * term_ratio +
            w4 * sentence_length_factor
    )
    return min(max(complex_word_ratio, 0.0), 1.0)

def calculate_formula_density(text):
    formula_pattern = re.compile(
        r"\\\[.*?\\\]|\$.*?\$|[A-Za-z]\w*\s*\(\w+\s*,\s*\w+\)\s*=\s*[^=]+|[A-Za-z]\w*\s*=\s*[^=]+|MHA\(.*?\)|softmax\(.*?\)|FFN\(.*?\)",
        re.DOTALL
    )
    formulas = formula_pattern.findall(text)
    formula_count = len(formulas)
    if formula_count == 0:
        return 0.0
    word_count = len(text.split())
    def calculate_formula_complexity(formula):
        score = 0.0
        if any(op in formula for op in ['+', '-', '*', '/']):
            score += 0.1
        score += 0.3 * (formula.count(r'\sum') + formula.count(r'\prod') +
                        formula.count(r'\int') + formula.count(r'\partial'))
        if formula.count(r'\sum') > 1 or r'{' in formula:
            score += 0.2
        score += 0.1 * (formula.count(r'\delta') + formula.count(r'\mu') +
                        formula.count(r'_') + formula.count(r'^'))
        if 'softmax' in formula or 'MHA' in formula or 'FFN' in formula:
            score += 0.3
        return min(1.0, score)
    formula_score = 0.0
    for formula in formulas:
        complexity = calculate_formula_complexity(formula)
        formula_score += complexity * 0.6
    base_scaling_factor = 5.0
    normalized_text_length = max(math.log(word_count + 1), 1) * base_scaling_factor
    raw_density = formula_score / normalized_text_length
    scaled_density = math.log(1 + raw_density)
    scaled_density = scaled_density / (1 + scaled_density)
    return scaled_density

def generate_ngrams(tokens, min_n, max_n):
    ngrams = set()
    for n in range(min_n, max_n + 1):
        for i in range(len(tokens) - n + 1):
            ngrams.add(' '.join(tokens[i:i + n]))
    return ngrams

def calculate_knowledge_abstractness(text, term_bank):
    tokens = word_tokenize(text.lower())
    ngrams = generate_ngrams(tokens, 1, 3)
    matched_terms = ngrams & term_bank.terms
    total_ngrams = len(ngrams)
    term_density = len(matched_terms) / max(total_ngrams, 1)
    weighted_term_score = 0
    for term in matched_terms:
        n_words = len(term.split())
        if n_words == 1:
            weight = 1.0
        elif n_words == 2:
            weight = 1.5
        else:
            weight = 2.0
        weighted_term_score += weight
    weighted_term_score = weighted_term_score / max(total_ngrams, 20)
    w1, w2 = 0.5, 0.5
    knowledge_abstractness = w1 * term_density + w2 * weighted_term_score
    return min(max(knowledge_abstractness, 0.0), 1.0)

def calculate_structure_disorder(text):
    section_patterns = [
        r'(?i)\b(Chapter|Part|Section|Module|Unit)\s+[\w\d.]+\b',
        r'(?i)\b\d+\.\d+\s+[A-Z][a-z]+\b',
        r'^#{1,3}\s+.+$'
    ]
    section_count = sum(len(re.findall(p, text)) for p in section_patterns)
    word_count = len(text.split())
    length_factor = min(1.0, word_count / 200)
    sentences = sent_tokenize(text)
    avg_sentence_length = np.mean([len(s.split()) for s in sentences]) if sentences else 0
    newline_count = text.count("\n")
    jumpiness_score = abs(len(sentences) - avg_sentence_length)
    jumpiness_factor = min(1.0, (jumpiness_score + newline_count) / 15)
    section_factor = 1 / (section_count + 1)
    structure_disorder = (
            section_factor * 0.2 +
            length_factor * 0.3 +
            jumpiness_factor * 0.5
    )
    final_score = round(max(0.1, min(structure_disorder, 1.0)), 2)
    return final_score

# ========================================
# ğŸ“¥ è¾“å…¥æ¨¡å—
# ========================================

class InputModule:
    def __init__(self, cognitive_load_path, score_path, textbook_path, term_bank_path="academic_terms.txt"):
        self.term_bank = AcademicTermBank(term_bank_path)
        self.cognitive_load_data = self.load_cognitive_load(cognitive_load_path)
        self.score_data = self.load_scores(score_path)
        self.textbook_data = self.load_textbook(textbook_path)

    def load_cognitive_load(self, path):
        try:
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            df = pd.DataFrame(data)
            if 'responses' not in df.columns or df['responses'].apply(lambda x: len(x) != 8).any():
                raise ValueError("è®¤çŸ¥è´Ÿè·æ•°æ®æ ¼å¼é”™è¯¯ï¼šæ¯ä¸ªå­¦ç”Ÿå¿…é¡»æœ‰ 8 ä¸ªè¯„åˆ†")
            df['has_valid_responses'] = df['responses'].apply(lambda x: any(v is not None for v in x))
            if not df['has_valid_responses'].any():
                print("è­¦å‘Šï¼šæ‰€æœ‰è®¤çŸ¥è´Ÿè·æ•°æ®å‡ä¸º nullï¼Œä½¿ç”¨é»˜è®¤å€¼")
                df['cognitive_load_level'] = 'medium'
                df['cognitive_load_score'] = 50.0
            else:
                df[['cognitive_load_level', 'cognitive_load_score']] = df['responses'].apply(self.calculate_cognitive_load).apply(pd.Series)
            return df
        except Exception as e:
            raise ValueError(f"åŠ è½½è®¤çŸ¥è´Ÿè·æ•°æ®å¤±è´¥: {str(e)}")

    def calculate_cognitive_load(self, responses):
        valid_responses = [r for r in responses if r is not None]
        if not valid_responses:
            return pd.Series(['medium', 50.0])
        total_score = sum(valid_responses)
        normalized_score = (total_score - len(valid_responses)) / (5 * len(valid_responses) - len(valid_responses)) * 100
        if normalized_score >= 70:
            return pd.Series(['high', normalized_score])
        elif normalized_score >= 40:
            return pd.Series(['medium', normalized_score])
        else:
            return pd.Series(['low', normalized_score])

    def load_scores(self, path):
        try:
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            df = pd.DataFrame(data)
            if 'score' not in df.columns or df['score'].isnull().any():
                raise ValueError("æˆç»©æ•°æ®æ ¼å¼é”™è¯¯ï¼šæ¯ä¸ªå­¦ç”Ÿå¿…é¡»æœ‰æœ‰æ•ˆæˆç»©")
            df[['score_level', 'score']] = df['score'].apply(self.classify_score).apply(pd.Series)
            return df
        except Exception as e:
            raise ValueError(f"åŠ è½½æˆç»©æ•°æ®å¤±è´¥: {str(e)}")

    def classify_score(self, score):
        if score >= 85:
            return pd.Series(['high', score])
        elif score >= 50:
            return pd.Series(['medium', score])
        else:
            return pd.Series(['low', score])

    def load_textbook(self, path):
        try:
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            if not isinstance(data, list):
                raise ValueError("æ•™ææ•°æ®å¿…é¡»ä¸º JSON æ•°ç»„")
            return data
        except Exception as e:
            raise ValueError(f"åŠ è½½æ•™ææ•°æ®å¤±è´¥: {str(e)}")

# ========================================
# ğŸ§  è¯„ä¼°æ¨¡å—
# ========================================

class EvaluationModule:
    def __init__(self, model_path='best_model_xgb.pkl', scaler_path='scaler.pkl', weights_path='weights_xgb.pkl'):
        self.model = joblib.load(model_path)
        self.scaler = joblib.load(scaler_path)
        self.weights_info = joblib.load(weights_path)
        self.term_bank = AcademicTermBank()
        self.table_analyzer = TableComplexityAnalyzer()

    def evaluate_textbook_difficulty(self, textbook_data):
        features = []
        for entry in textbook_data:
            text = entry.get("text", "")
            linguistic_complexity = calculate_language_complexity(text, self.term_bank)
            formula_density = calculate_formula_density(text)
            diagram_complexity = self.table_analyzer.analyze_entry(entry)
            knowledge_abstraction = calculate_knowledge_abstractness(text, self.term_bank)
            structural_disorganization = calculate_structure_disorder(text)
            feature_dict = {
                'linguistic_complexity': linguistic_complexity,
                'formula_density': formula_density,
                'diagram_complexity': diagram_complexity,
                'knowledge_abstraction': knowledge_abstraction,
                'structural_disorganization': structural_disorganization,
                'formula_knowledge_interaction': formula_density * knowledge_abstraction,
                'complex_knowledge_interaction': linguistic_complexity * knowledge_abstraction,
                'has_formula': int(formula_density > 0),
                'has_diagram': int(diagram_complexity > 0),
                'has_structural_disorganization': int(structural_disorganization > 0)
            }
            features.append(feature_dict)
        features_df = pd.DataFrame(features)
        feature_names_model_order = [
            'complex_word_ratio', 'formula_density', 'knowledge_abstractness',
            'structure_disorder', 'table_complexity',
            'formula_knowledge_interaction', 'complex_knowledge_interaction',
            'has_formula', 'has_table', 'has_structure_disorder'
        ]
        feature_name_mapping = {
            'complex_word_ratio': 'linguistic_complexity',
            'knowledge_abstractness': 'knowledge_abstraction',
            'structure_disorder': 'structural_disorganization',
            'table_complexity': 'diagram_complexity',
            'has_table': 'has_diagram',
            'has_structure_disorder': 'has_structural_disorganization'
        }
        for model_feature_name in feature_names_model_order:
            if model_feature_name in feature_name_mapping:
                features_df[model_feature_name] = features_df[feature_name_mapping[model_feature_name]]
        X = features_df[feature_names_model_order].values
        if np.any(np.isnan(X)):
            print("è­¦å‘Šï¼šç‰¹å¾åŒ…å«ç¼ºå¤±å€¼ï¼Œä½¿ç”¨ä¸­ä½æ•°å¡«å……")
            for i in range(X.shape[1]):
                X[:, i] = np.where(np.isnan(X[:, i]), np.nanmedian(X[:, i]), X[:, i])
        formula_idx_model = feature_names_model_order.index('formula_density')
        X[:, formula_idx_model] = np.log1p(X[:, formula_idx_model])
        table_idx_model = feature_names_model_order.index('table_complexity')
        bins = self.weights_info.get('table_complexity_bins', [0, 0.1, 0.5, 1.0, 2.0])
        cut_result = pd.cut(X[:, table_idx_model], bins=bins, labels=False, include_lowest=True)
        if np.any(np.isnan(X[:, table_idx_model])):
            print(f"è­¦å‘Šï¼šåœ¨è¿›è¡Œ table_complexity åˆ†ç®±å‰å‘ç° NaN å€¼ï¼Œä½¿ç”¨ä¸­ä½æ•°å¡«å……")
            original_vals = X[:, table_idx_model]
            median_val = np.nanmedian(original_vals)
            X[:, table_idx_model] = np.where(np.isnan(original_vals), median_val, original_vals)
            cut_result = pd.cut(X[:, table_idx_model], bins=bins, labels=False, include_lowest=True)
        X[:, table_idx_model] = cut_result.astype(np.float64)
        X_std = self.scaler.transform(X)
        difficulty_scores = self.model.predict(X_std)
        all_zero_mask = np.all(X[:, :5] == 0, axis=1)
        difficulty_scores[all_zero_mask] = 1.0
        features_df['difficulty_score'] = np.round(difficulty_scores, 1)
        overall_difficulty = features_df['difficulty_score'].mean()
        return features_df, overall_difficulty

# ========================================
# ğŸ¯ IRT è‡ªé€‚åº”è°ƒèŠ‚
# ========================================

class IRTOptimizedLearningAdaptation:
    def __init__(self):
        self.D = 1.702
        self.a = 1.7
        self.b_original_min = 1.0
        self.b_original_max = 10.0
        self.b_mapped_min = 1.0
        self.b_mapped_max = 5.0

    def _map_difficulty_scale(self, b_original):
        b_mapped = self.b_mapped_min + \
                   (b_original - self.b_original_min) / (self.b_original_max - self.b_original_min) * \
                   (self.b_mapped_max - self.b_mapped_min)
        return b_mapped

    def calculate_ability(self, score_level, score, cognitive_load_level, cognitive_load_score):
        if score < 50:
            theta_score = 2.0
        elif score >= 85:
            theta_score = 4.5
        else:
            theta_score = 2.0 + (score - 50) / (85 - 50) * (4.5 - 2.0)
        theta_load = 0.0
        if cognitive_load_level == 'high':
            theta_load = -1.0
        elif cognitive_load_level == 'low':
            theta_load = +1.0
        theta = theta_score + theta_load
        return max(min(theta, 5.0), 1.0)

    def calculate_probability(self, theta, b_mapped):
        return 1 / (1 + np.exp(-self.D * self.a * (theta - b_mapped)))

    def adjust_difficulty(self, delta, P_theta):
        if delta >= 2.0 and P_theta < 0.12:
            return 'significant_downgrade', 'å¤§å¹…é™ä½è¯­è¨€å¤æ‚åº¦ï¼Œç®€åŒ–ç»“æ„'
        elif 1.0 <= delta < 2.0 and 0.12 <= P_theta < 0.27:
            return 'moderate_downgrade', 'é€‚åº¦é™ä½è¯­è¨€å¤æ‚åº¦ï¼Œå¢åŠ ç¤ºä¾‹'
        elif -1.0 <= delta < 1.0 and 0.27 <= P_theta <= 0.73:
            return 'maintain', 'ä¼˜åŒ–å›¾è¡¨ï¼Œè¡¥å……ç»ƒä¹ '
        elif delta < -1.0 and P_theta >= 0.73:
            return 'upgrade', 'å¢åŠ å¼€æ”¾æ€§é—®é¢˜ï¼Œå¢å¼ºæŠ½è±¡åº¦'
        elif delta >= 1.0 and P_theta >= 0.27 or (-1.0 <= delta < 2.0 and P_theta < 0.12):
            return 'slight_downgrade', 'ç•¥å¾®é™ä½è¯­è¨€å¤æ‚åº¦ï¼Œå¢åŠ è¾…åŠ©è¯´æ˜'
        elif delta < -1.0 and P_theta < 0.73 or (-2.0 <= delta < 1.0 and P_theta > 0.73):
            return 'slight_upgrade', 'ç•¥å¾®å¢åŠ å†…å®¹æ·±åº¦ï¼Œå¼•å…¥ç®€å•æŒ‘æˆ˜'
        else:
            return 'slight_adjust', 'å¾®è°ƒå†…å®¹ç»“æ„ï¼Œä¿æŒç°æœ‰éš¾åº¦'

    def predict_optimal_challenge(self, student_ability, overall_difficulty_original):
        b_mapped = self._map_difficulty_scale(overall_difficulty_original)
        P_theta = self.calculate_probability(student_ability, b_mapped)
        delta = b_mapped - student_ability
        adjustment, suggestion = self.adjust_difficulty(delta, P_theta)
        return {
            'difficulty_score': round(b_mapped, 2),
            'P_theta': round(P_theta, 2),
            'delta': round(delta, 2),
            'adjustment': adjustment,
            'suggestion': suggestion
        }

# ========================================
# ğŸ“š ä¸»ç³»ç»Ÿ
# ========================================

class TextbookDifficultySystem:
    def __init__(self, cognitive_load_path, score_path, textbook_path, model_path='best_model_xgb.pkl',
                 scaler_path='scaler.pkl', weights_path='weights_xgb.pkl', term_bank_path='academic_terms.txt',
                 minimax_api_key=None):
        self.input_module = InputModule(cognitive_load_path, score_path, textbook_path, term_bank_path)
        self.evaluation_module = EvaluationModule(model_path, scaler_path, weights_path)
        self.irt_olad = IRTOptimizedLearningAdaptation()
        self.minimax_api_key = minimax_api_key or "your_minimax_api_key"

    def generate_new_textbook(self, textbook_snippet, features, theta, delta, adjustment, suggestion):
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªæ•™è‚²æœºå™¨äººï¼Œä»»åŠ¡æ˜¯ç”Ÿæˆä¸å­¦ç”Ÿèƒ½åŠ›åŒ¹é…çš„ä¸ªæ€§åŒ–æ•™æå†…å®¹ã€‚åŸºäºä»¥ä¸‹ä¿¡æ¯ï¼š
        - åŸæ•™æå†…å®¹ï¼š{textbook_snippet}
        - äº”ç»´éš¾æ˜“åº¦åˆ†æ•°ï¼š
          - è¯­è¨€å¤æ‚æ€§ï¼š{features['linguistic_complexity']}
          - å…¬å¼å¯†åº¦ï¼š{features['formula_density']}
          - è§†è§‰å¤æ‚æ€§ï¼š{features['diagram_complexity']}
          - çŸ¥è¯†æŠ½è±¡åº¦ï¼š{features['knowledge_abstraction']}
          - ç»“æ„æ— åºåº¦ï¼š{features['structural_disorganization']}
          - ç»¼åˆéš¾åº¦ï¼š{features['difficulty_score']}
        - å­¦ç”Ÿèƒ½åŠ›ï¼ˆÎ¸ï¼‰ï¼š{theta}
        - èƒ½åŠ›ä¸éš¾åº¦å·®å€¼ï¼ˆÎ”ï¼‰ï¼š{delta}
        - è°ƒæ•´ç­–ç•¥ï¼š{suggestion}
        ç”Ÿæˆä¸€ä¸ªæ–°çš„æ•™æç‰‡æ®µï¼ˆçº¦100-200å­—ï¼‰ï¼Œä¸åŸæ•™æä¸»é¢˜ç›¸å…³ï¼Œæ ¼å¼ä¸ºJSONï¼š
        {{
          "text": "æ–°æ•™æå†…å®¹",
          "image_path": ""
        }}
        è§„åˆ™ï¼š
        - å¦‚æœÎ”â‰¥2ï¼ˆsignificant_downgradeï¼‰ï¼šç®€åŒ–è¯­è¨€ï¼ˆé¿å…æœ¯è¯­ï¼‰ï¼Œç§»é™¤å…¬å¼ï¼Œé™ä½æŠ½è±¡åº¦ã€‚
        - å¦‚æœ1â‰¤Î”<2ï¼ˆmoderate_downgradeï¼‰ï¼šä½¿ç”¨ç®€å•æªè¾ï¼Œå¢åŠ 1-2ä¸ªç¤ºä¾‹ï¼Œå‡å°‘å…¬å¼ã€‚
        - å¦‚æœ-1â‰¤Î”<1ï¼ˆmaintainï¼‰ï¼šä¿æŒéš¾åº¦ï¼Œä¼˜åŒ–ç»“æ„ï¼Œæ·»åŠ æ¸…æ™°æ ‡é¢˜ã€‚
        - å¦‚æœÎ”<-1ï¼ˆupgradeï¼‰ï¼šå¢åŠ æŠ½è±¡æ¦‚å¿µï¼Œå¼•å…¥1ä¸ªç®€å•å…¬å¼ï¼Œä¿æŒæ¸…æ™°ç»“æ„ã€‚
        - å¦‚æœå…¬å¼å¯†åº¦>0.3ï¼Œå‡å°‘å…¬å¼ï¼›å¦‚æœè¯­è¨€å¤æ‚æ€§>0.5ï¼Œç®€åŒ–æªè¾ï¼›å¦‚æœçŸ¥è¯†æŠ½è±¡åº¦>0.4ï¼Œå‡å°‘æŠ½è±¡æœ¯è¯­ã€‚
        """
        headers = {
            "Authorization": f"Bearer {self.minimax_api_key}",
            "Content-Type": "application/json"
        }
        data = {
            "model": "abab6.5s-chat",
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": 500,
            "temperature": 0.7
        }
        response = requests.post("https://api.minimax.chat/v1/text/chatcompletion", headers=headers, json=data)
        response_data = response.json()
        try:
            new_textbook = json.loads(response_data['choices'][0]['message']['content'])
        except:
            new_textbook = {"text": response_data['choices'][0]['message']['content'], "image_path": ""}
        return new_textbook

    def run(self, output_path=None):
        student_data = pd.merge(
            self.input_module.cognitive_load_data[['student_id', 'cognitive_load_level', 'cognitive_load_score']],
            self.input_module.score_data[['student_id', 'score_level', 'score']],
            on='student_id'
        )
        textbook_data = self.input_module.textbook_data
        features_df, overall_difficulty_original = self.evaluation_module.evaluate_textbook_difficulty(textbook_data)

        results = []
        new_textbooks = []
        for _, student in student_data.iterrows():
            theta = self.irt_olad.calculate_ability(
                student['score_level'], student['score'],
                student['cognitive_load_level'], student['cognitive_load_score']
            )
            challenge = self.irt_olad.predict_optimal_challenge(theta, overall_difficulty_original)
            challenge['student_id'] = student['student_id']
            challenge['cognitive_load_level'] = student['cognitive_load_level']
            challenge['cognitive_load_score'] = student['cognitive_load_score']
            challenge['score'] = student['score']
            challenge['match_status'] = "åŒ¹é…" if -1 <= challenge['delta'] <= 1 else ("éœ€é™ä½éš¾åº¦" if challenge['delta'] > 1 else "éœ€æå‡æŒ‘æˆ˜")
            results.append(challenge)

            new_textbook = self.generate_new_textbook(
                textbook_snippet=textbook_data[0]['text'],
                features=features_df.iloc[0].to_dict(),
                theta=theta,
                delta=challenge['delta'],
                adjustment=challenge['adjustment'],
                suggestion=challenge['suggestion']
            )
            new_textbooks.append({
                'student_id': student['student_id'],
                'new_textbook': new_textbook
            })

        result_df = pd.DataFrame(results)
        result_df = result_df[[
            'student_id', 'cognitive_load_level', 'cognitive_load_score', 'score',
            'difficulty_score', 'delta', 'P_theta', 'adjustment', 'suggestion', 'match_status'
        ]].rename(columns={
            'difficulty_score': 'æ•™æéš¾æ˜“åº¦',
            'delta': 'Î”èŒƒå›´',
            'P_theta': 'P(Î¸)åŒºé—´',
            'adjustment': 'è°ƒèŠ‚ç­‰çº§',
            'suggestion': 'æ“ä½œå»ºè®®',
            'cognitive_load_score': 'è®¤çŸ¥è´Ÿè·å¾—åˆ†',
            'match_status': 'åŒ¹é…çŠ¶æ€'
        })

        output_json = {
            "textbook_features": features_df[[
                'difficulty_score', 'linguistic_complexity', 'formula_density',
                'diagram_complexity', 'knowledge_abstraction', 'structural_disorganization'
            ]].to_dict(orient='records'),
            "overall_difficulty": round(overall_difficulty_original, 2),
            "students": result_df.to_dict(orient='records'),
            "new_textbooks": new_textbooks
        }

        if output_path is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_path = f'results_{timestamp}.json'
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_json, f, ensure_ascii=False, indent=2)
        print(f"ç»“æœå·²ä¿å­˜è‡³ {output_path}")

        return output_json

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="æ•™æéš¾æ˜“åº¦é¢„æµ‹ä¸è°ƒèŠ‚ç³»ç»Ÿ")
    parser.add_argument('--cognitive_load', type=str, default='cognitive_load.json', help="è®¤çŸ¥è´Ÿè·æ•°æ® JSON æ–‡ä»¶è·¯å¾„")
    parser.add_argument('--scores', type=str, default='scores.json', help="æˆç»©æ•°æ® JSON æ–‡ä»¶è·¯å¾„")
    parser.add_argument('--textbook', type=str, default='textbook.json', help="æ•™ææ•°æ® JSON æ–‡ä»¶è·¯å¾„")
    parser.add_argument('--output', type=str, default='results.json', help="è¾“å‡º JSON æ–‡ä»¶è·¯å¾„")
    parser.add_argument('--model', type=str, default='best_model_xgb.pkl', help="XGBoostæ¨¡å‹è·¯å¾„")
    parser.add_argument('--scaler', type=str, default='scaler.pkl', help="æ ‡å‡†åŒ–å™¨è·¯å¾„")
    parser.add_argument('--weights', type=str, default='weights_xgb.pkl', help="æƒé‡æ–‡ä»¶è·¯å¾„")
    parser.add_argument('--term_bank', type=str, default='academic_terms.txt', help="æœ¯è¯­åº“æ–‡ä»¶è·¯å¾„")
    parser.add_argument('--minimax_api_key', type=str, required=True, help="MiniMax API å¯†é’¥")
    args = parser.parse_args()

    system = TextbookDifficultySystem(
        cognitive_load_path=args.cognitive_load,
        score_path=args.scores,
        textbook_path=args.textbook,
        model_path=args.model,
        scaler_path=args.scaler,
        weights_path=args.weights,
        term_bank_path=args.term_bank,
        minimax_api_key=args.minimax_api_key
    )
    result = system.run(args.output)
    print(json.dumps(result, indent=2, ensure_ascii=False))
